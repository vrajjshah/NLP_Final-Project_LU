{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP PROJECT",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QbZUj67qIqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing various Libraries\n",
        "%tensorflow_version 1.x\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import gensim\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eHXxGzIqVba",
        "colab_type": "code",
        "outputId": "b157a120-e9e2-4afa-f7f5-ea6ff4f6e1b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "#Import dataset by using pandas dataframe and then printing first 10 rows of it\n",
        "df =  pd.read_csv('https://storage.googleapis.com/tensorflow-workshop-examples/stack-overflow-data.csv')\n",
        "df = df[pd.notnull(df['tags'])]\n",
        "print(df.head(10))\n",
        "print(df['post'].apply(lambda x: len(x.split(' '))).sum())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                post           tags\n",
            "0  what is causing this behavior  in our c# datet...             c#\n",
            "1  have dynamic html load as if it was in an ifra...        asp.net\n",
            "2  how to convert a float value in to min:sec  i ...    objective-c\n",
            "3  .net framework 4 redistributable  just wonderi...           .net\n",
            "4  trying to calculate and print the mean and its...         python\n",
            "5  how to give alias name for my website  i have ...        asp.net\n",
            "6  window.open() returns null in angularjs  it wo...      angularjs\n",
            "7  identifying server timeout quickly in iphone  ...         iphone\n",
            "8  unknown method key  error in rails 2.3.8 unit ...  ruby-on-rails\n",
            "9  from the include  how to show and hide the con...      angularjs\n",
            "10286120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8jIAVAvq1Os",
        "colab_type": "code",
        "outputId": "86365ccc-358b-463c-90b9-e5ff3b35e073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "#Plotting the Classes(X-axis) and their no. of occurences(y-axis) in the dataset\n",
        "my_tags = ['java','html','asp.net','c#','ruby-on-rails','jquery','mysql','php','ios','javascript','python','c','css','android','iphone','sql','objective-c','c++','angularjs','.net']\n",
        "plt.figure(figsize=(10,4))\n",
        "df.tags.value_counts().plot(kind='bar');"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlwAAAEuCAYAAABbHsznAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAbgy5ZR0+v9XAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48WmDsfYq6ix",
        "colab_type": "code",
        "outputId": "4ac8f621-dc31-4323-ab1a-344e16f46bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#Printing the question(posts)-tags pairs\n",
        "def print_plot(index):\n",
        "    example = df[df.index == index][['post', 'tags']].values[0]\n",
        "    if len(example) > 0:\n",
        "        print(example[0])\n",
        "        print('Tag:', example[1])\n",
        "\n",
        "print_plot(30)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "how to chain expressions inside ngclass when using the {...}[] form  how can i add another expression to an <code>ng-class</code> directive that uses this form:   <pre><code>ng-class= {true: loading   false: loading-done }[data.loader===null]  </code></pre>   i d like to add something like this to the list:   <pre><code>{highlight:isspecial} </code></pre>   is it possible without expanding the first expression     thanks.\n",
            "Tag: angularjs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQB69qxXrNpJ",
        "colab_type": "code",
        "outputId": "87d238d8-a3f7-470e-d7f3-608cfce18a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#text pre-processing\n",
        "nltk.download('stopwords')\n",
        "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text\n",
        "    \n",
        "df['post'] = df['post'].apply(clean_text)\n",
        "print_plot(10)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "need interface c# possible duplicate would want use interfaces need interface want know use example interface idemo function prototype public void show first class using interface class myclass1 idemo public void show function body comes responsewrite myclass second class using interface class myclass2 idemo public void show function body comes responsewrite myclass2 responsewrite two classes function name different body even achieved without interface need interface use\n",
            "Tag: c#\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwH_2MezJ3ll",
        "colab_type": "code",
        "outputId": "d5cd4e71-46fa-47b5-b34a-77ab2c41e96f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Ccounting number of words in our dataset after cleaning\n",
        "df['post'].apply(lambda x: len(x.split(' '))).sum()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3424297"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pki07czKH83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splitting the dataset into training and testing\n",
        "X = df.post\n",
        "y = df.tags\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 2003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmzUZrU_sT77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining functions for evaluation metrics\n",
        "def recall_m(y_true, y_pred):\n",
        "  true_positives= K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  possible_positives= K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "  recall= true_positives / (possible_positives + K.epsilon())\n",
        "  return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "  true_positives= K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  predicted_positives= K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "  precision= true_positives / (predicted_positives + K.epsilon())\n",
        "  return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "  precision= precision_m(y_true, y_pred)\n",
        "  recall= recall_m(y_true, y_pred)\n",
        "  return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMpKUyCZKQMp",
        "colab_type": "text"
      },
      "source": [
        "Naiive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_XIE7A6KJGb",
        "colab_type": "code",
        "outputId": "a92bfc96-fc13-49d3-da1f-51196c5e503c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=my_tags))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.7455\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         java       0.58      0.64      0.61       597\n",
            "         html       0.90      0.87      0.89       572\n",
            "      asp.net       0.92      0.91      0.91       626\n",
            "           c#       0.74      0.72      0.73       624\n",
            "ruby-on-rails       0.71      0.87      0.78       589\n",
            "       jquery       0.76      0.48      0.59       618\n",
            "        mysql       0.78      0.73      0.75       608\n",
            "          php       0.72      0.88      0.79       601\n",
            "          ios       0.59      0.69      0.64       576\n",
            "   javascript       0.64      0.63      0.64       618\n",
            "       python       0.64      0.60      0.62       575\n",
            "            c       0.78      0.79      0.79       595\n",
            "          css       0.83      0.60      0.70       604\n",
            "      android       0.72      0.81      0.76       585\n",
            "       iphone       0.65      0.82      0.73       604\n",
            "          sql       0.69      0.62      0.66       596\n",
            "  objective-c       0.83      0.77      0.80       601\n",
            "          c++       0.84      0.87      0.86       578\n",
            "    angularjs       0.91      0.93      0.92       616\n",
            "         .net       0.77      0.66      0.71       617\n",
            "\n",
            "     accuracy                           0.75     12000\n",
            "    macro avg       0.75      0.75      0.74     12000\n",
            " weighted avg       0.75      0.75      0.74     12000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5WlERLeKdcx",
        "colab_type": "text"
      },
      "source": [
        "Linear Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt26-Q59Kg9e",
        "colab_type": "code",
        "outputId": "7511bfc5-ace4-42dd-c9dc-6a75bf5cfe9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
        "               ])\n",
        "sgd.fit(X_train, y_train)\n",
        "\n",
        "y_pred = sgd.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=my_tags))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.7815\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         java       0.73      0.68      0.70       597\n",
            "         html       0.83      0.93      0.88       572\n",
            "      asp.net       0.90      0.95      0.93       626\n",
            "           c#       0.79      0.77      0.78       624\n",
            "ruby-on-rails       0.71      0.87      0.78       589\n",
            "       jquery       0.78      0.34      0.48       618\n",
            "        mysql       0.83      0.67      0.74       608\n",
            "          php       0.73      0.93      0.82       601\n",
            "          ios       0.77      0.56      0.65       576\n",
            "   javascript       0.74      0.57      0.64       618\n",
            "       python       0.70      0.66      0.68       575\n",
            "            c       0.78      0.85      0.81       595\n",
            "          css       0.77      0.78      0.78       604\n",
            "      android       0.81      0.87      0.84       585\n",
            "       iphone       0.81      0.81      0.81       604\n",
            "          sql       0.69      0.69      0.69       596\n",
            "  objective-c       0.80      0.89      0.84       601\n",
            "          c++       0.80      0.97      0.88       578\n",
            "    angularjs       0.84      0.96      0.89       616\n",
            "         .net       0.80      0.87      0.83       617\n",
            "\n",
            "     accuracy                           0.78     12000\n",
            "    macro avg       0.78      0.78      0.77     12000\n",
            " weighted avg       0.78      0.78      0.77     12000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9cIqcsRKmzj",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM0JSr4-Kp3P",
        "colab_type": "code",
        "outputId": "06162a7a-60b2-4e37-a042-f652ed3ed1f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
        "               ])\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=my_tags))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.7839166666666667\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         java       0.66      0.64      0.65       597\n",
            "         html       0.94      0.88      0.91       572\n",
            "      asp.net       0.98      0.94      0.96       626\n",
            "           c#       0.76      0.75      0.75       624\n",
            "ruby-on-rails       0.81      0.82      0.82       589\n",
            "       jquery       0.59      0.59      0.59       618\n",
            "        mysql       0.77      0.78      0.78       608\n",
            "          php       0.77      0.86      0.81       601\n",
            "          ios       0.67      0.69      0.68       576\n",
            "   javascript       0.65      0.62      0.63       618\n",
            "       python       0.61      0.66      0.63       575\n",
            "            c       0.83      0.83      0.83       595\n",
            "          css       0.81      0.78      0.80       604\n",
            "      android       0.86      0.86      0.86       585\n",
            "       iphone       0.83      0.80      0.82       604\n",
            "          sql       0.68      0.61      0.64       596\n",
            "  objective-c       0.83      0.83      0.83       601\n",
            "          c++       0.90      0.94      0.92       578\n",
            "    angularjs       0.94      0.96      0.95       616\n",
            "         .net       0.81      0.85      0.83       617\n",
            "\n",
            "     accuracy                           0.78     12000\n",
            "    macro avg       0.78      0.78      0.78     12000\n",
            " weighted avg       0.78      0.78      0.78     12000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHBd-GcjK5G6",
        "colab_type": "text"
      },
      "source": [
        "Word2vec and Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWHUUYGpK5vF",
        "colab_type": "code",
        "outputId": "734d3e83-4675-4d01-d294-b9a4f1875bb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I4joH7qYUVy",
        "colab_type": "code",
        "outputId": "09da3018-bacf-4caf-a4b9-98747df87c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import gensim\n",
        "wv = gensim.models.KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/Lakehead/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "wv.init_sims(replace=True)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtqSdww7bKaZ",
        "colab_type": "code",
        "outputId": "82bc1968-54e6-4541-e081-1335bb3e6b37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "from itertools import islice\n",
        "list(islice(wv.vocab, 13030, 13050))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Memorial_Hospital',\n",
              " 'Seniors',\n",
              " 'memorandum',\n",
              " 'elephant',\n",
              " 'Trump',\n",
              " 'Census',\n",
              " 'pilgrims',\n",
              " 'De',\n",
              " 'Dogs',\n",
              " '###-####_ext',\n",
              " 'chaotic',\n",
              " 'forgive',\n",
              " 'scholar',\n",
              " 'Lottery',\n",
              " 'decreasing',\n",
              " 'Supervisor',\n",
              " 'fundamentally',\n",
              " 'Fitness',\n",
              " 'abundance',\n",
              " 'Hold']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf5D4SthbQ67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_averaging(wv, words):\n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    for word in words:\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    if not mean:\n",
        "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
        "        # FIXME: remove these examples in pre-processing\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBhTnZt_bSIz",
        "colab_type": "code",
        "outputId": "a4e774bf-5947-4a4f-bf16-6e691b1bbcd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "def w2v_tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens\n",
        "    \n",
        "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
        "\n",
        "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
        "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
        "\n",
        "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
        "X_test_word_average = word_averaging_list(wv,test_tokenized)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "WARNING:root:cannot compute similarity with no input []\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr9OxvjObUNb",
        "colab_type": "code",
        "outputId": "ccd08180-0e06-4620-bca6-3b6ef274f596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg = logreg.fit(X_train_word_average, train['tags'])\n",
        "y_pred = logreg.predict(X_test_word_average)\n",
        "print('accuracy %s' % accuracy_score(y_pred, test.tags))\n",
        "print(classification_report(test.tags, y_pred,target_names=my_tags))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.6325\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         java       0.60      0.57      0.59       613\n",
            "         html       0.76      0.75      0.75       620\n",
            "      asp.net       0.64      0.65      0.65       587\n",
            "           c#       0.53      0.52      0.52       586\n",
            "ruby-on-rails       0.70      0.73      0.72       599\n",
            "       jquery       0.42      0.38      0.40       589\n",
            "        mysql       0.62      0.62      0.62       594\n",
            "          php       0.72      0.79      0.76       610\n",
            "          ios       0.57      0.62      0.59       617\n",
            "   javascript       0.54      0.54      0.54       587\n",
            "       python       0.59      0.50      0.54       611\n",
            "            c       0.61      0.61      0.61       594\n",
            "          css       0.63      0.60      0.62       619\n",
            "      android       0.59      0.57      0.58       574\n",
            "       iphone       0.69      0.74      0.72       584\n",
            "          sql       0.40      0.43      0.41       578\n",
            "  objective-c       0.67      0.70      0.69       591\n",
            "          c++       0.78      0.77      0.77       608\n",
            "    angularjs       0.84      0.81      0.82       638\n",
            "         .net       0.69      0.70      0.69       601\n",
            "\n",
            "     accuracy                           0.63     12000\n",
            "    macro avg       0.63      0.63      0.63     12000\n",
            " weighted avg       0.63      0.63      0.63     12000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMhAFm1xirzk",
        "colab_type": "text"
      },
      "source": [
        "Doc2vec and Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YhPcD1Pit_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "from gensim.models import Doc2Vec\n",
        "from sklearn import utils\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import re\n",
        "\n",
        "def label_sentences(corpus, label_type):\n",
        "    \"\"\"\n",
        "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
        "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
        "    a dummy index of the post.\n",
        "    \"\"\"\n",
        "    labeled = []\n",
        "    for i, v in enumerate(corpus):\n",
        "        label = label_type + '_' + str(i)\n",
        "        labeled.append(gensim.models.doc2vec.TaggedDocument(v.split(), [label]))\n",
        "    return labeled\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, test_size=0.3)\n",
        "X_train = label_sentences(X_train, 'Train')\n",
        "X_test = label_sentences(X_test, 'Test')\n",
        "all_data = X_train + X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwfEdZFvlAV-",
        "colab_type": "code",
        "outputId": "a1dc9716-f715-4531-b7ef-225dc740f79f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "all_data[:2]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['fulltext', 'search', 'php', 'pdo', 'returning', 'result', 'searched', 'lot', 'matter', 'find', 'wrong', 'setup', 'trying', 'fulltext', 'search', 'using', 'pdo', 'php', 'get', 'results', 'error', 'messages', 'table', 'contains', 'customer', 'details', 'id', 'int', '11', 'auto_increment', 'name', 'varchar', '150', 'lastname', 'varchar', '150', 'company', 'varchar', '250', 'adress', 'varchar', '150', 'postcode', 'int', '5', 'city', 'varchar', '150', 'email', 'varchar', '250', 'phone', 'varchar', '20', 'orgnr', 'varchar', '15', 'timestamp', 'timestamp', 'current_timestamp', 'run', 'sqlquery', 'alter', 'table', 'system_customer', 'add', 'fulltext', 'name', 'lastname', 'except', 'columns', 'id', 'postcode', 'timestamp', 'signs', 'trouble', 'far', 'idea', 'problem', 'lies', 'db', 'configuration', 'php', 'code', 'goes', 'php', 'sth', 'dbhprepare', 'select', 'name', 'lastname', 'company', 'adress', 'city', 'phone', 'email', 'orgnr', 'db_pre', 'customer', 'match', 'name', 'lastname', 'company', 'adress', 'city', 'phone', 'email', 'orgnr', 'search', 'boolean', 'mode', 'bind', 'placeholders', 'sthbindparam', 'search', 'data', 'sthexecute', 'rows', 'sthfetchall', 'testing', 'print_r', 'dbherrorinfo', 'empty', 'rows', 'echo', 'else', 'echo', 'foreach', 'rows', 'row', 'echo', 'tr', 'datahref', 'new_orderphp', 'cid', 'row', 'id', 'echo', 'td', 'row', 'name', 'td', 'echo', 'td', 'row', 'lastname', 'td', 'echo', 'td', 'row', 'company', 'td', 'echo', 'td', 'row', 'phone', 'td', 'echo', 'td', 'row', 'email', 'td', 'echo', 'td', 'date', 'ymd', 'strtotime', 'row', 'timestamp', 'td', 'echo', 'tr', 'echo', 'tried', 'change', 'parameter', 'searchquery', 'string', 'like', 'testcompany', 'somename', 'boolean', 'mode', 'also', 'read', 'word', 'found', '50', 'rows', 'counts', 'common', 'word', 'pretty', 'sure', 'case', 'uses', 'specific', 'words', 'table', 'uses', 'myisam', 'engine', 'get', 'results', 'error', 'messages', 'please', 'help', 'point', 'wrong', 'thank'], tags=['Train_0']),\n",
              " TaggedDocument(words=['select', 'everything', '1', 'table', 'x', 'rows', 'another', 'im', 'making', 'join', 'query', 'like', 'select', 'clothes', 'c', 'join', 'style', 'cstyleid', 'ssylelid', 'clothesid', '19', 'dont', 'want', 'select', 'everything', 'style', 'want', 'select', 'everything', 'clothes', '20', 'rows', 'select', '1', 'row', '10', 'style', 'easyest', 'way', 'without', 'select', 'every', 'row', 'clothes', '20', 'things', 'select', 'like', 'select', 'cid', 'cdescription', 'cname', 'csize', 'cbrand', 'sname', 'clothes', 'c', 'join', 'style', 'cstyleid', 'stsylelid', 'clothesid', '19', 'would', 'fastest', 'way', 'possibillity'], tags=['Train_1'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cszIoYzWlCoE",
        "colab_type": "code",
        "outputId": "b3c6ccc3-76a1-44d9-b669-5b3d6ee1a542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
        "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
        "\n",
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40000/40000 [00:00<00:00, 2324551.22it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2675024.08it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 3076525.41it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2629905.01it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2656261.93it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2844656.65it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 3107064.47it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2846442.37it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2374862.48it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 3213225.82it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 3041003.44it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2601199.42it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2828637.71it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2695264.99it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2168916.01it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2381233.11it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2637885.57it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2185529.34it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2715858.52it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2532754.03it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2539462.96it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2902030.03it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 3123318.19it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2913773.42it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2731333.50it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2842584.16it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 3168082.78it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2435751.97it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 3065227.46it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2659630.63it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2824447.14it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUEGbHssmxmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
        "    \"\"\"\n",
        "    Get vectors from trained doc2vec model\n",
        "    :param doc2vec_model: Trained Doc2Vec model\n",
        "    :param corpus_size: Size of the data\n",
        "    :param vectors_size: Size of the embedding vectors\n",
        "    :param vectors_type: Training or Testing vectors\n",
        "    :return: list of vectors\n",
        "    \"\"\"\n",
        "    vectors = np.zeros((corpus_size, vectors_size))\n",
        "    for i in range(0, corpus_size):\n",
        "        prefix = vectors_type + '_' + str(i)\n",
        "        vectors[i] = model.docvecs[prefix]\n",
        "    return vectors\n",
        "    \n",
        "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
        "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Di-W8VMm0zX",
        "colab_type": "code",
        "outputId": "f74f573e-c2d4-40e9-a405-bff43c9a9ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        }
      },
      "source": [
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg.fit(train_vectors_dbow, y_train)\n",
        "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
        "y_pred = logreg.predict(test_vectors_dbow)\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=my_tags))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.7985833333333333\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         java       0.68      0.66      0.67       589\n",
            "         html       0.88      0.90      0.89       661\n",
            "      asp.net       0.95      0.94      0.95       606\n",
            "           c#       0.80      0.77      0.78       613\n",
            "ruby-on-rails       0.84      0.88      0.86       601\n",
            "       jquery       0.70      0.69      0.70       585\n",
            "        mysql       0.87      0.80      0.83       621\n",
            "          php       0.81      0.82      0.82       587\n",
            "          ios       0.65      0.67      0.66       560\n",
            "   javascript       0.67      0.67      0.67       611\n",
            "       python       0.66      0.66      0.66       593\n",
            "            c       0.80      0.84      0.82       581\n",
            "          css       0.79      0.77      0.78       608\n",
            "      android       0.86      0.84      0.85       593\n",
            "       iphone       0.81      0.82      0.81       592\n",
            "          sql       0.69      0.65      0.67       597\n",
            "  objective-c       0.86      0.86      0.86       604\n",
            "          c++       0.90      0.93      0.91       610\n",
            "    angularjs       0.92      0.96      0.94       595\n",
            "         .net       0.79      0.82      0.81       593\n",
            "\n",
            "     accuracy                           0.80     12000\n",
            "    macro avg       0.80      0.80      0.80     12000\n",
            " weighted avg       0.80      0.80      0.80     12000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLZJfO8zm-kT",
        "colab_type": "text"
      },
      "source": [
        "BOW with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3857Fcx1nAgq",
        "colab_type": "code",
        "outputId": "a0fb8711-05bb-40c5-b428-22626e8b49e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import itertools\n",
        "import os\n",
        "%tensorflow_version 1.x\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "%tensorflow_version 1.x\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "%tensorflow_version 1.x\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import utils\n",
        "%tensorflow_version 1.x\n",
        "train_size = int(len(df) * .7)\n",
        "train_posts = df['post'][:train_size]\n",
        "train_tags = df['tags'][:train_size]\n",
        "\n",
        "test_posts = df['post'][train_size:]\n",
        "test_tags = df['tags'][train_size:]\n",
        "\n",
        "max_words = 1000\n",
        "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
        "tokenize.fit_on_texts(train_posts) # only fit on train\n",
        "\n",
        "x_train = tokenize.texts_to_matrix(train_posts)\n",
        "x_test = tokenize.texts_to_matrix(test_posts)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_tags)\n",
        "y_train = encoder.transform(train_tags)\n",
        "y_test = encoder.transform(test_tags)\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "y_train = utils.to_categorical(y_train, num_classes)\n",
        "y_test = utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy',f1_m,precision_m,recall_m])\n",
        "model.summary()              \n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_11 (Dense)             (None, 512)               512512    \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 20)                10260     \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 20)                0         \n",
            "=================================================================\n",
            "Total params: 522,772\n",
            "Trainable params: 522,772\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 25200 samples, validate on 2800 samples\n",
            "Epoch 1/20\n",
            "25200/25200 [==============================] - 7s 262us/step - loss: 1.0323 - acc: 0.7139 - f1_m: 0.5490 - precision_m: 0.5490 - recall_m: 0.5490 - val_loss: 0.6791 - val_acc: 0.7896 - val_f1_m: 0.7146 - val_precision_m: 0.7146 - val_recall_m: 0.7146\n",
            "Epoch 2/20\n",
            "25200/25200 [==============================] - 4s 155us/step - loss: 0.5717 - acc: 0.8173 - f1_m: 0.7546 - precision_m: 0.7546 - recall_m: 0.7546 - val_loss: 0.6560 - val_acc: 0.7932 - val_f1_m: 0.7336 - val_precision_m: 0.7336 - val_recall_m: 0.7336\n",
            "Epoch 3/20\n",
            "25200/25200 [==============================] - 4s 156us/step - loss: 0.4704 - acc: 0.8450 - f1_m: 0.7936 - precision_m: 0.7936 - recall_m: 0.7936 - val_loss: 0.6609 - val_acc: 0.7954 - val_f1_m: 0.7550 - val_precision_m: 0.7550 - val_recall_m: 0.7550\n",
            "Epoch 4/20\n",
            "25200/25200 [==============================] - 4s 155us/step - loss: 0.3948 - acc: 0.8708 - f1_m: 0.8250 - precision_m: 0.8250 - recall_m: 0.8250 - val_loss: 0.6979 - val_acc: 0.7811 - val_f1_m: 0.7493 - val_precision_m: 0.7493 - val_recall_m: 0.7493\n",
            "Epoch 5/20\n",
            "25200/25200 [==============================] - 4s 156us/step - loss: 0.3383 - acc: 0.8882 - f1_m: 0.8503 - precision_m: 0.8503 - recall_m: 0.8503 - val_loss: 0.7218 - val_acc: 0.7850 - val_f1_m: 0.7571 - val_precision_m: 0.7571 - val_recall_m: 0.7571\n",
            "Epoch 6/20\n",
            "25200/25200 [==============================] - 4s 156us/step - loss: 0.2822 - acc: 0.9071 - f1_m: 0.8763 - precision_m: 0.8763 - recall_m: 0.8763 - val_loss: 0.7423 - val_acc: 0.7879 - val_f1_m: 0.7596 - val_precision_m: 0.7596 - val_recall_m: 0.7596\n",
            "Epoch 7/20\n",
            "25200/25200 [==============================] - 4s 155us/step - loss: 0.2353 - acc: 0.9230 - f1_m: 0.8960 - precision_m: 0.8960 - recall_m: 0.8960 - val_loss: 0.7799 - val_acc: 0.7854 - val_f1_m: 0.7618 - val_precision_m: 0.7618 - val_recall_m: 0.7618\n",
            "Epoch 8/20\n",
            "25200/25200 [==============================] - 4s 156us/step - loss: 0.1979 - acc: 0.9360 - f1_m: 0.9135 - precision_m: 0.9135 - recall_m: 0.9135 - val_loss: 0.8139 - val_acc: 0.7800 - val_f1_m: 0.7614 - val_precision_m: 0.7614 - val_recall_m: 0.7614\n",
            "Epoch 9/20\n",
            "25200/25200 [==============================] - 4s 153us/step - loss: 0.1606 - acc: 0.9513 - f1_m: 0.9322 - precision_m: 0.9322 - recall_m: 0.9322 - val_loss: 0.8502 - val_acc: 0.7775 - val_f1_m: 0.7611 - val_precision_m: 0.7611 - val_recall_m: 0.7611\n",
            "Epoch 10/20\n",
            "25200/25200 [==============================] - 4s 153us/step - loss: 0.1357 - acc: 0.9587 - f1_m: 0.9437 - precision_m: 0.9437 - recall_m: 0.9437 - val_loss: 0.8934 - val_acc: 0.7789 - val_f1_m: 0.7625 - val_precision_m: 0.7625 - val_recall_m: 0.7625\n",
            "Epoch 11/20\n",
            "25200/25200 [==============================] - 4s 154us/step - loss: 0.1123 - acc: 0.9665 - f1_m: 0.9555 - precision_m: 0.9555 - recall_m: 0.9555 - val_loss: 0.9447 - val_acc: 0.7704 - val_f1_m: 0.7575 - val_precision_m: 0.7575 - val_recall_m: 0.7575\n",
            "Epoch 12/20\n",
            "25200/25200 [==============================] - 4s 154us/step - loss: 0.0924 - acc: 0.9722 - f1_m: 0.9637 - precision_m: 0.9637 - recall_m: 0.9637 - val_loss: 0.9663 - val_acc: 0.7704 - val_f1_m: 0.7593 - val_precision_m: 0.7593 - val_recall_m: 0.7593\n",
            "Epoch 13/20\n",
            "25200/25200 [==============================] - 4s 154us/step - loss: 0.0860 - acc: 0.9750 - f1_m: 0.9673 - precision_m: 0.9673 - recall_m: 0.9673 - val_loss: 0.9982 - val_acc: 0.7786 - val_f1_m: 0.7661 - val_precision_m: 0.7661 - val_recall_m: 0.7661\n",
            "Epoch 14/20\n",
            "25200/25200 [==============================] - 4s 156us/step - loss: 0.0717 - acc: 0.9795 - f1_m: 0.9739 - precision_m: 0.9739 - recall_m: 0.9739 - val_loss: 1.0397 - val_acc: 0.7746 - val_f1_m: 0.7618 - val_precision_m: 0.7618 - val_recall_m: 0.7618\n",
            "Epoch 15/20\n",
            "25200/25200 [==============================] - 4s 156us/step - loss: 0.0636 - acc: 0.9814 - f1_m: 0.9765 - precision_m: 0.9765 - recall_m: 0.9765 - val_loss: 1.0498 - val_acc: 0.7804 - val_f1_m: 0.7686 - val_precision_m: 0.7686 - val_recall_m: 0.7686\n",
            "Epoch 16/20\n",
            "25200/25200 [==============================] - 4s 155us/step - loss: 0.0585 - acc: 0.9845 - f1_m: 0.9801 - precision_m: 0.9801 - recall_m: 0.9801 - val_loss: 1.0808 - val_acc: 0.7800 - val_f1_m: 0.7725 - val_precision_m: 0.7725 - val_recall_m: 0.7725\n",
            "Epoch 17/20\n",
            "25200/25200 [==============================] - 4s 157us/step - loss: 0.0521 - acc: 0.9845 - f1_m: 0.9812 - precision_m: 0.9812 - recall_m: 0.9812 - val_loss: 1.1180 - val_acc: 0.7729 - val_f1_m: 0.7661 - val_precision_m: 0.7661 - val_recall_m: 0.7661\n",
            "Epoch 18/20\n",
            "25200/25200 [==============================] - 4s 154us/step - loss: 0.0491 - acc: 0.9859 - f1_m: 0.9831 - precision_m: 0.9831 - recall_m: 0.9831 - val_loss: 1.1437 - val_acc: 0.7811 - val_f1_m: 0.7718 - val_precision_m: 0.7718 - val_recall_m: 0.7718\n",
            "Epoch 19/20\n",
            "25200/25200 [==============================] - 4s 154us/step - loss: 0.0430 - acc: 0.9877 - f1_m: 0.9853 - precision_m: 0.9853 - recall_m: 0.9853 - val_loss: 1.1697 - val_acc: 0.7757 - val_f1_m: 0.7675 - val_precision_m: 0.7675 - val_recall_m: 0.7675\n",
            "Epoch 20/20\n",
            "25200/25200 [==============================] - 4s 155us/step - loss: 0.0417 - acc: 0.9876 - f1_m: 0.9859 - precision_m: 0.9859 - recall_m: 0.9859 - val_loss: 1.1961 - val_acc: 0.7811 - val_f1_m: 0.7746 - val_precision_m: 0.7746 - val_recall_m: 0.7746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhnwQFY3nW3S",
        "colab_type": "code",
        "outputId": "b86edeb9-cda1-4180-ff55-a7450348e327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss, accuracy, f1_score, precision, recall= model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
        "print(f'Test loss:{loss}, accuracy:{accuracy}, F-1 score:{f1_score}, Precision:{precision}, recall:{recall}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss:1.1811138429641723, accuracy:0.77875, F-1 score:0.7709166070620219, Precision:0.7709166666666667, recall:0.7709166666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTBDoKEaseQ8",
        "colab_type": "text"
      },
      "source": [
        "CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUN7pLJM4UEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3,\n",
        "                 activation='relu',\n",
        "                 input_shape=(1000,1)))\n",
        "model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
        "model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=1))\n",
        "model.add(Dropout(rate = 0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=['accuracy',f1_m,precision_m,recall_m])\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7rzF7wfFQzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "646c6433-8986-4139-dd2d-2006cbeee2f3"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28000, 1000, 1)\n",
            "(28000, 20)\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_5 (Conv1D)            (None, 998, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 994, 128)          41088     \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 990, 128)          82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 990, 128)          0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 990, 128)          0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 126720)            0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                8110144   \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 20)                1300      \n",
            "=================================================================\n",
            "Total params: 8,234,836\n",
            "Trainable params: 8,234,836\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpY4vid_FfyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "7d6f59e3-ae51-4e0c-c150-23c06046cd07"
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "          batch_size=32,validation_split=0.1,epochs=3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 66\n",
            "Train on 25200 samples, validate on 2800 samples\n",
            "Epoch 1/3\n",
            "25200/25200 [==============================] - 15s 591us/step - loss: 1.2300 - acc: 0.6408 - f1_m: 0.5100 - precision_m: 0.5100 - recall_m: 0.5100 - val_loss: 0.7103 - val_acc: 0.7750 - val_f1_m: 0.6968 - val_precision_m: 0.6968 - val_recall_m: 0.6968\n",
            "Epoch 2/3\n",
            "25200/25200 [==============================] - 12s 468us/step - loss: 0.8047 - acc: 0.7510 - f1_m: 0.6660 - precision_m: 0.6660 - recall_m: 0.6660 - val_loss: 0.6719 - val_acc: 0.7950 - val_f1_m: 0.7193 - val_precision_m: 0.7193 - val_recall_m: 0.7193\n",
            "Epoch 3/3\n",
            "25200/25200 [==============================] - 12s 471us/step - loss: 0.6945 - acc: 0.7779 - f1_m: 0.7059 - precision_m: 0.7059 - recall_m: 0.7059 - val_loss: 0.6785 - val_acc: 0.7950 - val_f1_m: 0.7125 - val_precision_m: 0.7125 - val_recall_m: 0.7125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0S0yvn8GKhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras \n",
        "model.save('cnn.h5')\n",
        "model = keras.models.load_model('cnn.h5', \n",
        "                   custom_objects = {'f1_m': f1_m,  'precision_m': precision_m, 'recall_m' : recall_m})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYyBXyYLGaQd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d17f79f2-12c3-44ad-e5d5-357300b7e00a"
      },
      "source": [
        "loss, accuracy, f1_score, precision, recall= model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
        "print(f'Test loss:{loss}, accuracy:{accuracy}, F-1 score:{f1_score}, Precision:{precision}, recall:{recall}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss:0.6748847924868265, accuracy:0.7883333333333333, F-1 score:0.7067499403953552, Precision:0.70675, recall:0.70675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1LSZOPiaGky",
        "colab_type": "text"
      },
      "source": [
        "HYBRID CNN then LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8fowPgWb4iW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "958d2787-a188-45bb-e967-4f251dcf4e1b"
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import LSTM\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3,\n",
        "                 activation='relu',\n",
        "                 input_shape=(1000,1)))\n",
        "model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=1))\n",
        "model.add(LSTM(units=64, return_sequences=True, name='output'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
        "              metrics=['accuracy',f1_m,precision_m,recall_m])\n",
        "\n",
        "print(model.summary())\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "model.fit(x_train, y_train,batch_size=128, epochs=3, verbose=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_3 (Conv1D)            (None, 998, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 994, 128)          41088     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 994, 128)          0         \n",
            "_________________________________________________________________\n",
            "output (LSTM)                (None, 994, 64)           49408     \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 63616)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                4071488   \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 20)                1300      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 20)                0         \n",
            "=================================================================\n",
            "Total params: 4,163,540\n",
            "Trainable params: 4,163,540\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "(28000, 1000, 1)\n",
            "(28000, 20)\n",
            "Epoch 1/3\n",
            "28000/28000 [==============================] - 400s 14ms/step - loss: 1.3958 - acc: 0.5840 - f1_m: 0.4440 - precision_m: 0.4440 - recall_m: 0.4440\n",
            "Epoch 2/3\n",
            "28000/28000 [==============================] - 399s 14ms/step - loss: 0.7981 - acc: 0.7568 - f1_m: 0.6753 - precision_m: 0.6753 - recall_m: 0.6753\n",
            "Epoch 3/3\n",
            "28000/28000 [==============================] - 394s 14ms/step - loss: 0.6920 - acc: 0.7832 - f1_m: 0.7113 - precision_m: 0.7113 - recall_m: 0.7113\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1b5d0429e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "653MblGMTwW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras \n",
        "model.save('hybrid.h5')\n",
        "model = keras.models.load_model('hybrid.h5', \n",
        "                   custom_objects = {'f1_m': f1_m,  'precision_m': precision_m, 'recall_m' : recall_m})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhqZhUpnT9ze",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5516ab3f-c1c2-4d09-bded-b969902af6ba"
      },
      "source": [
        "loss, accuracy, f1_score, precision, recall= model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
        "print(f'Test loss:{loss}, accuracy:{accuracy}, F-1 score:{f1_score}, Precision:{precision}, recall:{recall}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss:0.6629065762758255, accuracy:0.7871666666666667, F-1 score:0.7289166070620219, Precision:0.7289166666666667, recall:0.7289166666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLd8girF_9VZ",
        "colab_type": "text"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyJ2knUN3qHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "0a1c1c49-c546-4b85-d3f6-59e2d3713a21"
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import LSTM\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=128, input_shape = (1000,1), return_sequences = True))\n",
        "model.add(LSTM(units=64, return_sequences=True, name='output'))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
        "              metrics=['accuracy',f1_m,precision_m,recall_m])\n",
        "\n",
        "print(model.summary())\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "model.fit(x_train, y_train,batch_size=128, epochs=3, verbose=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (None, 1000, 128)         66560     \n",
            "_________________________________________________________________\n",
            "output (LSTM)                (None, 1000, 64)          49408     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 64000)             0         \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 64000)             0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64000)             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 20)                1280020   \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 20)                0         \n",
            "=================================================================\n",
            "Total params: 1,395,988\n",
            "Trainable params: 1,395,988\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "(28000, 1000, 1)\n",
            "(28000, 20)\n",
            "Epoch 1/3\n",
            "28000/28000 [==============================] - 766s 27ms/step - loss: 1.6499 - acc: 0.5041 - f1_m: 0.3360 - precision_m: 0.3360 - recall_m: 0.3360\n",
            "Epoch 2/3\n",
            "28000/28000 [==============================] - 754s 27ms/step - loss: 0.9927 - acc: 0.6970 - f1_m: 0.5938 - precision_m: 0.5938 - recall_m: 0.5938\n",
            "Epoch 3/3\n",
            "28000/28000 [==============================] - 759s 27ms/step - loss: 0.8088 - acc: 0.7520 - f1_m: 0.6672 - precision_m: 0.6672 - recall_m: 0.6672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1b9e3de710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU9oLMkpKS2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras \n",
        "model.save('lstm.h5')\n",
        "model = keras.models.load_model('lstm.h5', \n",
        "                   custom_objects = {'f1_m': f1_m,  'precision_m': precision_m, 'recall_m' : recall_m})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN0EQbosKYER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b8115be-ff17-4788-8db6-e2ebf7b5dda1"
      },
      "source": [
        "loss, accuracy, f1_score, precision, recall= model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
        "print(f'Test loss:{loss}, accuracy:{accuracy}, F-1 score:{f1_score}, Precision:{precision}, recall:{recall}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss:0.8402288181781769, accuracy:0.746, F-1 score:0.6621666070620219, Precision:0.6621666666666667, recall:0.6621666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}